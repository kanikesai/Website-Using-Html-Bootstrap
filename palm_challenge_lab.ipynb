{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEqbX8OhE8y9"
   },
   "source": [
    "# PaLM Challenge Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VK1Q5ZYdVL4Y"
   },
   "source": [
    "## Overview\n",
    "\n",
    "You've just spent some time running through many of the elements related to Google's PaLM API, its use and its API. This challenge lab is designed to test your knoledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQT500QqVPIb",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Objectives\n",
    "\n",
    "- Load the Vertex AI language models\n",
    "- Authenticate your environment\n",
    "- Demonstrate the use of prompt design, ideation, text classification, and text extraction\n",
    "\n",
    "Most of the following Python notebook cells have missing or incomplete code sections. Your challenge is to complete each cell, run it to test for correctness, and then move on. When all the cells are working, you have completed the challenge.\n",
    "\n",
    "**Note: The notebooks used in the PaLM labs may all be viewed directly on Github [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDU0XJ1xRDlL"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5afkyDMSBW5"
   },
   "source": [
    "### Install Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc4WxYmLSBW5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Complete the following pip command\n",
    "!pip install google-cloud-aiplatform --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart your notebook Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Fom0ZkMSBW6"
   },
   "source": [
    "### Authenticating your notebook environment\n",
    "* Use the instructions found [here](https://github.com/GoogleCloudPlatform/generative-ai/tree/main/setup-env) in the block below, insert the lines required to set the project ID and location (us-central1 works for sure) variables. Then import and initialize the vertexai library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCaCx6PLSBW6"
   },
   "outputs": [],
   "source": [
    "# insert the requisite steps here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuQwwRiniVFG"
   },
   "source": [
    "### Import libraries\n",
    "Import the following language model classes:\n",
    "- ChatModel\n",
    "- InputOutputTextPair\n",
    "- TextEmbeddingModel\n",
    "- TextGenerationModel\n",
    "\n",
    "from the Vertex AI, preview, language models module. \n",
    "\n",
    "For display purposes, also import Markdown and display from IPython display. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4zjV4alsiVql"
   },
   "outputs": [],
   "source": [
    "# Complete the two imports\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting Google's PaLM technology to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4437b7608c8e"
   },
   "source": [
    "#### Load the TextGenerationModel, and store it in a the `generation_model` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2998506fe6d1"
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kEAJ0ipmbndQ"
   },
   "source": [
    "### Prompt design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCgBDJvNRCF5"
   },
   "source": [
    "Create a prediction around the prompt, \"Tell me about Google's PaLM API.\" Set the `temperature` for the least open ended response ans set the `max_output_tokens` for the longest response possible with the text-bison@001 model. Leave the top_k and top_p with their defaults. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cx_o455SRCF5"
   },
   "outputs": [],
   "source": [
    "prompt = \"Tell me about Google's PaLM API.\"\n",
    "response = generation_model.predict(\n",
    "            prompt, temperature=1, max_output_tokens=1024\n",
    "        )\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EsglQtgDRCF5"
   },
   "source": [
    "### Ideation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BP1BKWiRCF6"
   },
   "source": [
    "Use the below template to get your model to give you 5 title ideas for a training course on Google's Generative AI technologies. Use display and Markdown to render the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2USfPyOuFhlB"
   },
   "outputs": [],
   "source": [
    "prompt = \"Generate 5 title ideas for a training course on Google's Generative AI technologies.\"\n",
    "response = generation_model.predict(\n",
    "                prompt, temperature=0.2, max_output_tokens=1024, top_k=40, top_p=0.8\n",
    "        ).text\n",
    "display(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification\n",
    "Let's try a language classification problem. Using the below starting code, determine the language of: \"Es viernes todavía.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "determine the language of:\n",
    "text: \"Es viernes todavía.\"\n",
    "language:\n",
    "\"\"\"\n",
    "print(\n",
    "generation_model.predict(\n",
    "                prompt=prompt\n",
    "            ).text\n",
    ")\n",
    "\n",
    "# add code to print the prediction using the defaults for temperature, max output tokens, top_p and k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are a little wordy, use one-shot prompting to get the prediction to return a single word to you, the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Given the following text, classify the language it is written in.\n",
    "\n",
    "Replace this with the one-shot\n",
    "\n",
    "text: Es viernes todaví?\n",
    "language:\n",
    "\"\"\"\n",
    "\n",
    "print(\n",
    "    generation_model.predict(\n",
    "        prompt=prompt,\n",
    "    ).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Extraction\n",
    "Convert the following list of cooking ingredients to a YAML file with the keys ingredient, quantity, type\n",
    "\n",
    "Ingredients\n",
    "* 9 egg whites\n",
    "* 3/8 tsp Cream of Tartar\n",
    "* 1 1/2 tbs Viniger\n",
    "* 1 1/2 tsp Vanilla\n",
    "* 3 cups Sugar\n",
    "* 1 quarts Heavy whipping cream\n",
    "* 3 boxes Strawberries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Extract the ingredients from the following recipe. Return the ingredients to a YAML file with the keys: ingredient, quantity, type.\n",
    "\n",
    "Ingredients:\n",
    "*9 egg whites\n",
    "*3/8 tsp Cream of Tartar\n",
    "*1 1/2 tbs Viniger\n",
    "*1 1/2 tsp Vanilla\n",
    "*3 cups Sugar\n",
    "*1 quarts Heavy whipping cream\n",
    "*3 boxes Strawberries\n",
    "\"\"\"\n",
    "print(\n",
    "generation_model.predict(\n",
    "                prompt, temperature=0.2, max_output_tokens=1024, top_k=40, top_p=0.8\n",
    "           ).text\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent work. You have now demonstrated your ability to use many key features in Google's PaLM library. Nice job.We likely want some nice wrapup here, but I don't know what, ha"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_palm_api.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-11.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-11:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
